test2 <- function(data){
reps <- numeric(R)
for (i in 1:R) {
k <- sample(K,size = N,replace = FALSE)
y <- data[k,3]
reps[i] <- t(data$x,data$m,y)
}
p_hat <- (sum(abs(reps)>abs(t(data$x,data$m,data$y)))+1)/(1+R)
return(p_hat<0.05)
}
test3 <- function(data){
reps <- numeric(R)
for (i in 1:R) {
k <- sample(K,size = N,replace = FALSE)
m <- data[k,2]
reps[i] <- t(data$x,m,data$y)
}
p_hat <- (sum(abs(reps)>abs(t(data$x,data$m,data$y)))+1)/(1+R)
return(p_hat<0.05)
}
#data generation
model <- function(alpha,beta,gamma,N){
e_M <- rnorm(N,0,1)
e_Y <- rnorm(N,0,1)
x <- rnorm(N,0,1)
m <- alpha*x+e_M
y <- beta*m+gamma*x+e_Y
return(data.frame(x,m,y))
}
t <- function(x,m,y){              #Test statistics
data <- data.frame(x,m,y)
f1 <- lm(m~x,data=data)
f2 <- lm(y~m+x,data=data)
s_alpha <- summary(f1)$coefficients[2,2]
s_beta <- summary(f2)$coefficients[2,2]
t <- f1$coef[2]*f2$coef[2]/sqrt(f1$coef[2]^2*s_alpha^2+f2$coef[2]^2*s_beta^2)
return(as.numeric(t))
}
N <- 20
R <- 299
K <- 1:N
test1 <- function(data){
reps <- numeric(R)
for (i in 1:R) {
k <- sample(K,size = N,replace = FALSE)
x <- data[k,1]
reps[i] <- t(x,data$m,data$y)
}
p_hat <- (sum(abs(reps)>abs(t(data$x,data$m,data$y)))+1)/(1+R)
return(p_hat<0.05)
}
test2 <- function(data){
reps <- numeric(R)
for (i in 1:R) {
k <- sample(K,size = N,replace = FALSE)
y <- data[k,3]
reps[i] <- t(data$x,data$m,y)
}
p_hat <- (sum(abs(reps)>abs(t(data$x,data$m,data$y)))+1)/(1+R)
return(p_hat<0.05)
}
test3 <- function(data){
reps <- numeric(R)
for (i in 1:R) {
k <- sample(K,size = N,replace = FALSE)
m <- data[k,2]
reps[i] <- t(data$x,m,data$y)
}
p_hat <- (sum(abs(reps)>abs(t(data$x,data$m,data$y)))+1)/(1+R)
return(p_hat<0.05)
}
stimulation <- function(alpha,beta,gamma=1,N=20){
type_1_error <- matrix(0,nrow = 3,ncol = 100)
for (i in 1:100) {
data <- model(alpha,beta,gamma,N)
type_1_error[1,i] <- test1(data)
type_1_error[2,i] <- test2(data)
type_1_error[3,i] <- test3(data)
}
return(c(mean(type_1_error[1,]),mean(type_1_error[2,]),mean(type_1_error[3,])))
}
s1 <- stimulation(alpha=0,beta=0)
s2 <- stimulation(alpha=0,beta=1)
s3 <- stimulation(alpha=1,beta=0)
#data generation
model <- function(alpha,beta,gamma,N){
e_M <- rnorm(N,0,1)
e_Y <- rnorm(N,0,1)
x <- rnorm(N,0,1)
m <- alpha*x+e_M
y <- beta*m+gamma*x+e_Y
return(data.frame(x,m,y))
}
t <- function(x,m,y){              #Test statistics
data <- data.frame(x,m,y)
f1 <- lm(m~x,data=data)
f2 <- lm(y~m+x,data=data)
s_alpha <- summary(f1)$coefficients[2,2]
s_beta <- summary(f2)$coefficients[2,2]
t <- f1$coef[2]*f2$coef[2]/sqrt(f1$coef[2]^2*s_alpha^2+f2$coef[2]^2*s_beta^2)
return(as.numeric(t))
}
N <- 10
R <- 199
K <- 1:N
test1 <- function(data){
reps <- numeric(R)
for (i in 1:R) {
k <- sample(K,size = N,replace = FALSE)
x <- data[k,1]
reps[i] <- t(x,data$m,data$y)
}
p_hat <- (sum(abs(reps)>abs(t(data$x,data$m,data$y)))+1)/(1+R)
return(p_hat<0.05)
}
test2 <- function(data){
reps <- numeric(R)
for (i in 1:R) {
k <- sample(K,size = N,replace = FALSE)
y <- data[k,3]
reps[i] <- t(data$x,data$m,y)
}
p_hat <- (sum(abs(reps)>abs(t(data$x,data$m,data$y)))+1)/(1+R)
return(p_hat<0.05)
}
test3 <- function(data){
reps <- numeric(R)
for (i in 1:R) {
k <- sample(K,size = N,replace = FALSE)
m <- data[k,2]
reps[i] <- t(data$x,m,data$y)
}
p_hat <- (sum(abs(reps)>abs(t(data$x,data$m,data$y)))+1)/(1+R)
return(p_hat<0.05)
}
stimulation <- function(alpha,beta,gamma=1,N=10){
type_1_error <- matrix(0,nrow = 3,ncol = 50)
for (i in 1:50) {
data <- model(alpha,beta,gamma,N)
type_1_error[1,i] <- test1(data)
type_1_error[2,i] <- test2(data)
type_1_error[3,i] <- test3(data)
}
return(c(mean(type_1_error[1,]),mean(type_1_error[2,]),mean(type_1_error[3,])))
}
s1 <- stimulation(alpha=0,beta=0)
s2 <- stimulation(alpha=0,beta=1)
s3 <- stimulation(alpha=1,beta=0)
f <- function(f0,N=10^6,b1=0,b2=1,b3=-1){
x1 <- rpois(N,1)
x2 <- rexp(N,1)
x3 <- rbeta(N,1,0.5)
g <- function(alpha){
tmp <- exp(alpha+b1*x1+b2*x2+b3*x3)
p <- 1/(1+tmp)
mean(p) - f0
}
solution <- uniroot(g,c(-100,100))
alpha <- solution$root
return(alpha)
}
f0 <- c(0.1,0.01,0.001,0.0001)
alpha <- numeric(4)
for (i in 1:4) {
alpha[i] <- f(f0[i])
}
plot(-log(f0),alpha)
#data
U <- c(11,8,27,13,16,0,23,10,24,2)
V <- c(12,9,28,14,17,1,24,11,25,3)
# maximizing likelihood function
likelihood <- function(lambda){
return(sum((V*exp(-lambda*V)-U*exp(-lambda*U))/(exp(-lambda*U)-exp(-lambda*V))))
}
round(uniroot(likelihood, c(0, 10))$root, 5)
#EM-algorithm
lambda <- 3
lambda_t <- 2
i <- 0
eps <- 1e-3
while(1){
lambda_t <- lambda_t - likelihood(lambda)/length(U)
if (abs(1/lambda_t - lambda) < eps) break
lambda <- 1/lambda_t
i <- i+1
}
lambda
gibbsR <- function(N){
X <- matrix(0, N, 2) #the chain, a bivariate sample
rho <- 0.9 #correlation
mu1 <- 0
mu2 <- 0
sigma1 <- 1
sigma2 <- 1
s1 <- sqrt(1-rho^2)*sigma1
s2 <- sqrt(1-rho^2)*sigma2
X[1, ] <- c(mu1, mu2) #initialize
for (i in 2:N) {
x2 <- X[i-1, 2]
m1 <- mu1 + rho * (x2 - mu2) * sigma1/sigma2
X[i, 1] <- rnorm(1, m1, s1)
x1 <- X[i, 1]
m2 <- mu2 + rho * (x1 - mu1) * sigma2/sigma1
X[i, 2] <- rnorm(1, m2, s2)
}
return(X)
}
library(Rcpp)
cppFunction('NumericMatrix gibbsC(int N) {
double rho = 0.9;
double mu1 = 0;
double mu2 = 0;
double sigma1 = 1;
double sigma2 = 1;
double s1 = 0.4358899;
double s2 = 0.4358899;
NumericMatrix X(N, 2);
X(0,0) = mu1;
X(0,1) = mu2;
for (int i=1;i<N;i++) {
double x2 = X(i-1,1);
double m1 = mu1 + rho * (x2 - mu2) * sigma1/sigma2;
X(i,0) = rnorm(1, m1, s1)[0];
double x1 = X(i,0);
double m2 = mu2 + rho * (x1 - mu1) * sigma2/sigma1;
X(i,1) = rnorm(1, m2, s2)[0];
}
return(X);
}')
GibbsR <- gibbsR(2000)
GibbsC <- gibbsC(2000)
qqplot(GibbsR,GibbsC)
library(StatComp22019)
library(microbenchmark)
data(document)
document <- document[1:50]
usethis::use_data(document)
usethis::use_data(document)
library(StatComp22019)
library(microbenchmark)
data(document)
lda_pack <- function(data,K){
vid<-Corpus(VectorSource(data))
vid<-tm_map(vid,stripWhitespace)
#创建文本-词条矩阵
dtm <- DocumentTermMatrix(vid,
control = list(
wordLengths=c(2, Inf),               #限制词长
bounds = list(global = c(5,Inf)),    #设置词的最小频率
removeNumbers = TRUE,                #是否移除数字
weighting = weightTf,                #词频率权重
encoding = "UTF-8"))
Gibbs = LDA(dtm,k = K,method = "Gibbs")
return(Gibbs)
}
tm2 <- microbenchmark(
ldar = LDAR(document,13,10),
ldap = lda_pack(document,13)
)
tm2 <- microbenchmark(
ldar = LDAR(document,7,10),
ldap = lda_pack(document,7)
)
document <- document[1:10]
usethis::use_data(document)
ldar = LDAR(document,9,10)
devtools::document()
devtools::check()
devtools::check()
library(StatComp22019)
library(microbenchmark)
data(document)
ldar = LDAR(document,5,10)
View(ldar)
library(jiebaR)#分词处理工具
library(lda)#LDA模型包
library(LDAvis)#LDA可视化
library(servr)
#library(Rcpp)
data <- readLines("sohu.txt",encoding = "UTF-8")
index <- sample(1:length(data),100)
data <- data[index]
stop_words<-readLines("stopwords.txt",encoding = "UTF-8")
wk = worker(stop_word='stopwords.txt',bylines = T)
fenci <- wk[data]
data2=gsub('[",c()-]', '', fenci)   #分词以及删除停词
library(jiebaR)#分词处理工具
library(lda)#LDA模型包
library(LDAvis)#LDA可视化
library(servr)
#library(Rcpp)
data <- readLines("sohu.txt",encoding = "UTF-8")
index <- sample(1:length(data),30)
data <- data[index]
stop_words<-readLines("stopwords.txt",encoding = "UTF-8")
wk = worker(stop_word='stopwords.txt',bylines = T)
fenci <- wk[data]
data2=gsub('[",c()-]', '', fenci)   #分词以及删除停词
document <- data2
usethis::use_data(document)
devtools::build_vignettes()
?Corpus
?tm_map
?DocumentTermMatrix
devtools::build_vignettes()
devtools::build_vignettes()
?LDA
detach("package:topicmodels", unload = TRUE)
library(topicmodels)
devtools::build_vignettes()
?LDAR
devtools::build_vignettes()
devtools::build_vignettes()
library(tm)
library(StatComp22019)
library(microbenchmark)
library(topicmodels)
data(document)
lda_pack <- function(data,K){
vid<-Corpus(VectorSource(data))
vid<-tm_map(vid,stripWhitespace)
#创建文本-词条矩阵
dtm <- DocumentTermMatrix(vid,
control = list(
wordLengths=c(2, Inf),               #限制词长
bounds = list(global = c(5,Inf)),    #设置词的最小频率
removeNumbers = TRUE,                #是否移除数字
weighting = weightTf,                #词频率权重
encoding = "UTF-8"))
Gibbs = LDA(dtm,k = K,method = "Gibbs")
return(Gibbs)
}
tm2 <- microbenchmark(
ldar = LDAR(document,9,10),
ldap = lda_pack(document,9)
)
devtools::build_vignettes()
?LDAR
LDAR(document,9,10)
devtools::build_vignettes()
devtools::document()
devtools::check()
devtools::check()
devtools::build_vignettes()
LDAR<- function(data,K,n){
vid<-Corpus(VectorSource(data))
vid<-tm_map(vid,stripWhitespace)
#创建文本-词条矩阵
dtm <- DocumentTermMatrix(vid,
control = list(
wordLengths=c(2, Inf),
bounds = list(global = c(5,Inf)),
removeNumbers = TRUE,
weighting = weightTf,
encoding = "UTF-8"))
Gibbs = LDA(dtm,k = K,method = "Gibbs")
return(terms(Gibbs,n))
}
LDAR(document,9)
LDAR(document,9,10)
devtools::build_vignettes()
devtools::build_vignettes()
devtools::build_vignettes()
devtools::build_vignettes()
devtools::build(vignettes=FALSE)
install.packages('../StatComp22019_1.0.tar.gz',repo=NULL)
devtools::document()
devtools::check()
devtools::check()
devtools::build_vignettes()
devtools::build(vignettes=FALSE)
devtools::build_vignettes()
devtools::document()
devtools::document()
devtools::check()
devtools::build_vignettes()
library(tm)
library(StatComp22019)
library(microbenchmark)
library(topicmodels)
data(document)
library(tm)
library(StatComp22019)
library(microbenchmark)
library(topicmodels)
data(document)
document <- document[1:10]
LDA_pkg<- function(data,K,n){
vid<-Corpus(VectorSource(data))
vid<-tm_map(vid,stripWhitespace)
#创建文本-词条矩阵
dtm <- DocumentTermMatrix(vid,
control = list(
wordLengths=c(2, Inf),
bounds = list(global = c(5,Inf)),
removeNumbers = TRUE,
weighting = weightTf,
encoding = "UTF-8"))
Gibbs = LDA(dtm,k = K,method = "Gibbs")
return(terms(Gibbs,n))
}
LDAR <- function(data,num_topic,num_word){
maxdoclen=0
for (i in 1:length(data)) {
line <- strsplit(data[i], "[[:space:]]+")
if (length(line[[1]])>maxdoclen)
maxdoclen = length(line[[1]])
}
tokens <- strsplit(data, "[[:space:]]+")
word_dict <- table(unlist(tokens))
word_dict <- sort(word_dict, decreasing = TRUE)
for (i in 1:length(word_dict)) {
word_dict[i]=i
}
vocasize=length(word_dict)#总词数
docnum=length(tokens)  #文档数
n_features = num_word
iter_times = 200
K=num_topic
beta = 0.1
alpha = 0.5
p <-  numeric(K)
nw <- matrix(0,nrow = vocasize,ncol = K)
nwsum <-  numeric(K)
nd <-  matrix(0,nrow = docnum,ncol = K)
ndsum <-  numeric(docnum)
z1 <- z2 <- matrix(0,nrow = docnum,ncol = maxdoclen)
theta <-  matrix(0,nrow = docnum,ncol = K)
phi1 <- phi2 <- matrix(0,nrow = K,ncol = vocasize)
# 初始化变量
for (i in 1:docnum) {
tokensi <- tokens[[i]]
ndsum[i] <- length(tokensi)
for (j in 1:length(tokensi)) {
word <- tokensi[j]
topic_index <- sample(1:K, 1)
word_id = as.numeric(word_dict[word])
z1[i,j] <- topic_index
z2[i,j] <- word_id
nw[word_id,topic_index] =nw[word_id,topic_index]+1   #topic k 下 word 出现次数
nd[i,topic_index] = nd[i,topic_index]+1              #文章d的topic k 的次数
nwsum[topic_index] = nwsum[topic_index]+1           #topic k的总次数
}
}
#Gibbs 采样
Vbeta = vocasize * beta
Kalpha = K * alpha
for (iter in 1:iter_times) {
#print(iter)
for (i in 1:docnum) {
for (j in 1:ndsum[i]) {
word_id = z2[i,j]
topic = z1[i,j]
nw[word_id,topic] =nw[word_id,topic]- 1
nd[i,topic] =nd[i,topic]- 1
nwsum[topic] = nwsum[topic]-1
for (k in 1:K) {
p[k] = (nw[word_id,k] + beta) / (nwsum[k] + Vbeta) * (nd[i,k] + alpha) / (ndsum[i] + Kalpha)
}
s <- rmultinom(1,1,p/sum(p))
t <- which(s==max(s))
nw[word_id,t] = nw[word_id,t]+1
nwsum[t] = nwsum[t]+1
nd[i,t] = nd[i,t]+1
z1[i,j] = t
z2[i,j] = word_id
}
}
}
for (i in 1:docnum) {
for (j in 1:K) {
theta[i,j] = (nd[i,j] + alpha) / (ndsum[i] + Kalpha)
}
}
for (i in 1:K) {
for (j in 1:vocasize) {
phi1[i,j]=(nw[j,i]+beta)/(ndsum[i]+Vbeta)
phi2[i,j]=j
}
}
result <- list()
for (i in 1:K) {
indexphi <- order(phi1[i,],decreasing = TRUE)
hotword<- character(length = n_features)
for (j in 1:n_features) {
topicword = word_dict[phi2[i,indexphi[j]]]
hotword[j] <- names(topicword)
}
result[[i]] <- hotword
}
return(result)
}
devtools::build_vignettes()
?microbenchmark
tm2 <- microbenchmark(
times = 10,
ldapkg = LDA_pkg(document,4,5),
ldar = LDAR(document,4,5)
)
tm2 <- microbenchmark(
times = 5,
ldapkg = LDA_pkg(document,4,5),
ldar = LDAR(document,4,5)
)
tm2 <- microbenchmark(
times = 1,
ldapkg = LDA_pkg(document,4,5),
ldar = LDAR(document,4,5)
)
knitr::kable(summary(tm2)[,c(1,3,5,6)])
devtools::build_vignettes()
devtools::build(vignettes=FALSE)
